{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EdvMVGhQY12x"},"outputs":[],"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = None\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n","!bash get_datasets.sh\n","%cd /content"]},{"cell_type":"markdown","metadata":{"tags":["pdf-title"],"id":"C9u17sPfY125"},"source":["# Image Captioning with RNNs\n","В этом упражнении вы создадите стандартные рекуррентные нейронные сети и будете использовать их для обучения модели, способной генерировать новые подписи к изображениям."]},{"cell_type":"markdown","metadata":{"id":"adtT8lWTY126"},"source":["## Install h5py\n","Набор данных COCO, который мы будем использовать, хранится в формате HDF5. Чтобы загрузить файлы HDF5, нам нужно установить пакет Python h5py. Из командной строки запустите: <br/>\n","`pip install h5py`  <br/>\n","Если вы получаете сообщение об ошибке прав доступа, вам может потребоваться запустить команду от имени пользователя root: <br/>\n","```sudo pip install h5py```\n","\n","Вы также можете запускать команды непосредственно из блокнота Jupyter, добавляя к команде префикс \"!\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Rn63RoVY127"},"outputs":[],"source":["!pip install h5py"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["pdf-ignore"],"id":"S_Y9o_-4Y128"},"outputs":[],"source":["# As usual, a bit of setup\n","import time, os, json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n","from cs231n.rnn_layers import *\n","from cs231n.captioning_solver import CaptioningSolver\n","from cs231n.classifiers.rnn import CaptioningRNN\n","from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n","from cs231n.image_utils import image_from_url\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"markdown","metadata":{"tags":["pdf-ignore"],"id":"TYYOv8s6Y129"},"source":["# Microsoft COCO\n","В этом упражнении мы будем использовать набор данных [Microsoft COCO](http://mscoco.org/) 2014 года, который стал стандартным испытательным стендом для подписей к изображениям. Набор данных состоит из 80 000 обучающих изображений и 40 000 проверочных изображений, каждое из которых снабжено 5 подписями, написанными работниками Amazon Mechanical Turk.\n","\n","\n","**Вы уже должны были загрузить данные, перейдя в каталог `cs231n/datasets` и запустив скрипт `get_assignment3_data.sh`. Если вы еще этого не сделали, запустите этот скрипт сейчас. Предупреждение: загрузка данных COCO составляет ~ 1 ГБ. **\n","\n","Мы уже предварительно обработали данные и извлекли для вас признаки. Для всех изображений мы извлекли признаки из слоя fc7 сети VGG-16, предварительно обученной в ImageNet; эти признаки хранятся в файлах `train2014_vgg16_fc7.h5` и `val2014_vgg16_fc7.h5` соответственно. Чтобы сократить время обработки и требования к памяти, мы уменьшили размерность признаков с 4096 до 512; эти признаки можно найти в файлах `train2014_vgg16_fc7_pca.h5` и `val2014_vgg16_fc7_pca.h5`.\n","\n","Необработанные изображения занимают много места (почти 20 ГБ), поэтому мы не включили их в загрузку. Однако все изображения взяты из Flickr, а URL-адреса обучающих и проверочных изображений хранятся в файлах `train2014_urls.txt` и `val2014_urls.txt` соответственно. Это позволяет загружать изображения на лету для визуализации. Поскольку изображения загружаются на лету, **вы должны быть подключены к Интернету для просмотра изображений**.\n","\n","Работа со строками неэффективна, поэтому мы будем работать с закодированной версией подписей. Каждому слову присваивается целочисленный ID, что позволяет нам представить подпись последовательностью целых чисел. Сопоставление между целочисленными ID и словами находится в файле `coco2014_vocab.json`, и вы можете использовать функцию `decode_captions` из файла `cs231n/coco_utils.py`, чтобы преобразовать массивы numpy целых идентификаторов обратно в строки.\n","\n","Есть пара специальных токенов, которые мы добавляем в словарь. Мы добавляем специальный токен `<START>` и добавляем токен `<END>` в начало и конец каждой подписи соответственно. Редкие слова заменяются специальным токеном `<UNK>` (для «неизвестного»). Кроме того, поскольку мы хотим тренироваться с мини-пакетами, содержащими подписи разной длины, мы дополняем короткие подписи специальным токеном `<NULL>` после токена `<END>` и не вычисляем потери или градиент для `<NULL>. ` токенов. Поскольку они немного утомительны, мы позаботились обо всех деталях реализации специальных токенов для вас.\n","\n","Вы можете загрузить все данные MS-COCO (подписи, признаки, URL-адреса и словарь), используя функцию `load_coco_data` из файла `cs231n/coco_utils.py`. Для этого запустите следующую ячейку:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["pdf-ignore"],"id":"QrZXsPvLY12-"},"outputs":[],"source":["# Load COCO data from disk; this returns a dictionary\n","# We'll work with dimensionality-reduced features for this notebook, but feel\n","# free to experiment with the original features by changing the flag below.\n","data = load_coco_data(pca_features=True)\n","\n","# Print out all the keys and values from the data dictionary\n","for k, v in data.items():\n","    if type(v) == np.ndarray:\n","        print(k, type(v), v.shape, v.dtype)\n","    else:\n","        print(k, type(v), len(v))"]},{"cell_type":"markdown","metadata":{"id":"_Vh-ddGfY12_"},"source":["## Look at the data\n","Всегда полезно просмотреть примеры из набора данных, прежде чем работать с ним.\n","\n","Вы можете использовать функцию `sample_coco_minibatch` из файла `cs231n/coco_utils.py` для выборки мини-пакетов данных из структуры данных, возвращенной из `load_coco_data`. Запустите следующее, чтобы отобрать небольшой пакет обучающих данных и отобразить изображения и подписи к ним. Запуск его множество раз и просмотр результатов поможет вам получить представление о наборе данных.\n","\n","Обратите внимание, что мы декодируем подписи с помощью функции `decode_captions` и загружаем изображения на лету, используя их URL-адрес Flickr, поэтому **вы должны быть подключены к Интернету для просмотра изображений**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSIoAi5kY13A"},"outputs":[],"source":["# Sample a minibatch and show the images and captions\n","batch_size = 3\n","\n","captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n","for i, (caption, url) in enumerate(zip(captions, urls)):\n","    plt.imshow(image_from_url(url))\n","    plt.axis('off')\n","    caption_str = decode_captions(caption, data['idx_to_word'])\n","    plt.title(caption_str)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GWErsKV9Y13C"},"source":["# Recurrent Neural Networks\n","\n","Как обсуждалось в лекции, мы будем использовать языковые модели рекуррентных нейронных сетей (RNN) для подписи к изображениям. Файл `cs231n/rnn_layers.py` содержит реализации различных типов слоев, необходимых для рекуррентных нейронных сетей, а файл `cs231n/classifiers/rnn.py` использует эти слои для реализации модели подписи изображений.\n","\n","Сначала мы реализуем различные типы слоев RNN в `cs231n/rnn_layers.py.`"]},{"cell_type":"markdown","metadata":{"id":"iEwcYDlEY13E"},"source":["# Vanilla RNN: step forward\n","\n","Откройте файл `cs231n/rnn_layers.py`. Этот файл реализует прямой и обратный проходы для различных типов слоев, которые обычно используются в рекуррентных нейронных сетях.\n","\n","Сначала реализуйте функцию `rnn_step_forward`, которая реализует прямой проход для одного временного шага обычной рекуррентной нейронной сети. После этого выполните следующее, чтобы проверить свою реализацию. Вы должны увидеть ошибки порядка e-8 или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZChVFkfNY13F"},"outputs":[],"source":["N, D, H = 3, 10, 4\n","\n","x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n","prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n","b = np.linspace(-0.2, 0.4, num=H)\n","\n","next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n","expected_next_h = np.asarray([\n","  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n","  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n","  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n","\n","print('next_h error: ', rel_error(expected_next_h, next_h))"]},{"cell_type":"markdown","metadata":{"id":"GqTIozB2Y13G"},"source":["# Vanilla RNN: step backward\n","\n","В файле `cs231n/rnn_layers.py` реализуем функцию `rnn_step_backward`. После этого запустите следующее, чтобы численно проверить вашу реализацию градиента. Вы должны увидеть ошибки порядка `e-8` или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRBARESUY13G"},"outputs":[],"source":["from cs231n.rnn_layers import rnn_step_forward, rnn_step_backward\n","np.random.seed(231)\n","N, D, H = 4, 5, 6\n","x = np.random.randn(N, D)\n","h = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b = np.random.randn(H)\n","\n","out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n","\n","dnext_h = np.random.randn(*out.shape)\n","\n","fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n","dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n","db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n","\n","dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"id":"nH_iNB9vY13H"},"source":["# Vanilla RNN: forward\n","\n","Теперь, когда вы реализовали прямой и обратный проходы для одного временного шага обычной RNN, вы объедините эти части для реализации RNN, которая обрабатывает всю последовательность данных.\n","\n","В файле `cs231n/rnn_layers.py` реализуйте функцию `rnn_forward`. Это должно быть реализовано с помощью функции `rnn_step_forward`, которую вы определили выше. После этого выполните следующее, чтобы проверить свою реализацию. Вы должны увидеть ошибки порядка `e-7` или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eACMc_McY13H"},"outputs":[],"source":["N, T, D, H = 2, 3, 4, 5\n","\n","x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n","h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n","Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n","Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n","b = np.linspace(-0.7, 0.1, num=H)\n","\n","h, _ = rnn_forward(x, h0, Wx, Wh, b)\n","expected_h = np.asarray([\n","  [\n","    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n","    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n","    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n","  ],\n","  [\n","    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n","    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n","    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n","print('h error: ', rel_error(expected_h, h))"]},{"cell_type":"markdown","metadata":{"id":"6Ln3WI3TY13I"},"source":["# Vanilla RNN: backward\n","\n","В файле `cs231n/rnn_layers.py` реализуйте обратный проход для ванильного RNN в функции `rnn_backward`. Это должно запустить обратное распространение по всей последовательности, вызывая функцию `rnn_step_backward`, которую вы определили ранее. Вы должны увидеть ошибки порядка e-6 или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjnllthAY13I"},"outputs":[],"source":["np.random.seed(231)\n","\n","N, D, T, H = 2, 3, 10, 5\n","\n","x = np.random.randn(N, T, D)\n","h0 = np.random.randn(N, H)\n","Wx = np.random.randn(D, H)\n","Wh = np.random.randn(H, H)\n","b = np.random.randn(H)\n","\n","out, cache = rnn_forward(x, h0, Wx, Wh, b)\n","\n","dout = np.random.randn(*out.shape)\n","\n","dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n","\n","fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n","fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dout)\n","dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n","dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n","dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dh0 error: ', rel_error(dh0_num, dh0))\n","print('dWx error: ', rel_error(dWx_num, dWx))\n","print('dWh error: ', rel_error(dWh_num, dWh))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"id":"xl5nltnsY13J"},"source":["# Word embedding: forward\n","\n","В системах глубокого обучения мы обычно представляем слова с помощью векторов. Каждое слово словаря будет связано с вектором, и эти векторы будут изучаться вместе с остальной частью системы.\n","\n","В файле `cs231n/rnn_layers.py` реализовать функцию` word_embedding_forward`, чтобы преобразовать слова (представленные целыми числами) в векторы. Запустите следующее, чтобы проверить свою реализацию. Вы должны увидеть ошибку порядка `e-8` или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HyHRVb7Y13J"},"outputs":[],"source":["N, T, V, D = 2, 4, 5, 3\n","\n","x = np.asarray([[0, 3, 1, 2], [2, 1, 0, 3]])\n","W = np.linspace(0, 1, num=V*D).reshape(V, D)\n","\n","out, _ = word_embedding_forward(x, W)\n","expected_out = np.asarray([\n"," [[ 0.,          0.07142857,  0.14285714],\n","  [ 0.64285714,  0.71428571,  0.78571429],\n","  [ 0.21428571,  0.28571429,  0.35714286],\n","  [ 0.42857143,  0.5,         0.57142857]],\n"," [[ 0.42857143,  0.5,         0.57142857],\n","  [ 0.21428571,  0.28571429,  0.35714286],\n","  [ 0.,          0.07142857,  0.14285714],\n","  [ 0.64285714,  0.71428571,  0.78571429]]])\n","\n","print('out error: ', rel_error(expected_out, out))"]},{"cell_type":"markdown","metadata":{"id":"bzElNYsHY13K"},"source":["# Word embedding: backward\n","\n","Реализуйте обратный проход для функции встраивания слов в функцию `word_embedding_backward`. После этого запустите следующее, чтобы численно проверить вашу реализацию градиента. Вы должны увидеть ошибку порядка `e-11` или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaNoLXPtY13K"},"outputs":[],"source":["np.random.seed(231)\n","\n","N, T, V, D = 50, 3, 5, 6\n","x = np.random.randint(V, size=(N, T))\n","W = np.random.randn(V, D)\n","\n","out, cache = word_embedding_forward(x, W)\n","dout = np.random.randn(*out.shape)\n","dW = word_embedding_backward(dout, cache)\n","\n","f = lambda W: word_embedding_forward(x, W)[0]\n","dW_num = eval_numerical_gradient_array(f, W, dout)\n","\n","print('dW error: ', rel_error(dW, dW_num))"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"-3CbnN-bY13K"},"source":["# Temporal Affine layer\n","\n","На каждом временном шаге мы используем аффинную функцию для преобразования скрытого вектора RNN на этом временном шаге в оценки для каждого слова в словаре. Поскольку это очень похоже на аффинный слой, который вы реализовали в assignment 2, мы предоставили вам эту функцию в функциях `temporal_affine_forward` и `temporal_affine_backward` в файле `cs231n/rnn_layers.py`. Выполните следующую команду, чтобы выполнить численную проверку реализации  градиента. Вы должны увидеть ошибки порядка e-9 или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"iPKZbbJHY13L"},"outputs":[],"source":["np.random.seed(231)\n","\n","# Gradient check for temporal affine layer\n","N, T, D, M = 2, 3, 4, 5\n","x = np.random.randn(N, T, D)\n","w = np.random.randn(D, M)\n","b = np.random.randn(M)\n","\n","out, cache = temporal_affine_forward(x, w, b)\n","\n","dout = np.random.randn(*out.shape)\n","\n","fx = lambda x: temporal_affine_forward(x, w, b)[0]\n","fw = lambda w: temporal_affine_forward(x, w, b)[0]\n","fb = lambda b: temporal_affine_forward(x, w, b)[0]\n","\n","dx_num = eval_numerical_gradient_array(fx, x, dout)\n","dw_num = eval_numerical_gradient_array(fw, w, dout)\n","db_num = eval_numerical_gradient_array(fb, b, dout)\n","\n","dx, dw, db = temporal_affine_backward(dout, cache)\n","\n","print('dx error: ', rel_error(dx_num, dx))\n","print('dw error: ', rel_error(dw_num, dw))\n","print('db error: ', rel_error(db_num, db))"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"-4dnZcNsY13L"},"source":["# Temporal Softmax loss\n","\n","В языковой модели RNN на каждом временном шаге мы получаем оценку для каждого слова в словаре. Мы знаем истинное слово на каждом временном шаге, поэтому мы используем функцию потерь softmax для вычисления потерь и градиента на каждом временном шаге. Мы суммируем потери по времени и усредняем их по мини-партии.\n","\n","Однако есть один нюанс: поскольку мы работаем с мини-пакетами и разные подписи могут иметь разную длину, мы добавляем токены `<NULL>` в конец каждой подписи, чтобы все они имели одинаковую длину. Мы не хотим, чтобы эти токены `<NULL>` учитывались в счет потери или градиента, поэтому в дополнение к оценкам и достоверным меткам (ground-truth)  наша функция потерь также принимает массив `mask`, который сообщает ей, какие элементы оценок учитываются в отношении потерь.\n","\n","Поскольку это очень похоже на функцию потерь softmax, которую вы реализовали в assignment 1, мы реализовали эту функцию потерь для вас; посмотрите на функцию `temporal_softmax_loss` в файле `cs231n/rnn_layers.py`.\n","\n","Запустите следующую ячейку, чтобы проверить потери и выполните численную проверку градиента для функции. Вы должны увидеть ошибку для dx порядка e-7 или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ZzbjNVDVY13M"},"outputs":[],"source":["# Sanity check for temporal softmax loss\n","from cs231n.rnn_layers import temporal_softmax_loss\n","\n","N, T, V = 100, 1, 10\n","\n","def check_loss(N, T, V, p):\n","    x = 0.001 * np.random.randn(N, T, V)\n","    y = np.random.randint(V, size=(N, T))\n","    mask = np.random.rand(N, T) <= p\n","    print(temporal_softmax_loss(x, y, mask)[0])\n","  \n","check_loss(100, 1, 10, 1.0)   # Should be about 2.3\n","check_loss(100, 10, 10, 1.0)  # Should be about 23\n","check_loss(5000, 10, 10, 0.1) # Should be within 2.2-2.4\n","\n","# Gradient check for temporal softmax loss\n","N, T, V = 7, 8, 9\n","\n","x = np.random.randn(N, T, V)\n","y = np.random.randint(V, size=(N, T))\n","mask = (np.random.rand(N, T) > 0.5)\n","\n","loss, dx = temporal_softmax_loss(x, y, mask, verbose=False)\n","\n","dx_num = eval_numerical_gradient(lambda x: temporal_softmax_loss(x, y, mask)[0], x, verbose=False)\n","\n","print('dx error: ', rel_error(dx, dx_num))"]},{"cell_type":"markdown","metadata":{"id":"glayvbwiY13M"},"source":["# RNN for image captioning\n","\n","Теперь, когда вы реализовали необходимые слои, вы можете объединить их для создания модели подписывания изображений. Откройте файл `cs231n/classifiers/rnn.py` и посмотрите на класс `CaptioningRNN`.\n","\n","Реализуйте прямой и обратный проход модели в функции `loss`. На данный момент вам нужно только реализовать случай, когда `cell_type='rnn'` для RNN vanialla; вы реализуете случай LSTM позже. После этого запустите следующее, чтобы проверить ваш прямой проход, используя небольшой тестовый пример; вы должны увидеть ошибку порядка `e-10` или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"w5YymMNCY13P"},"outputs":[],"source":["N, D, W, H = 10, 20, 30, 40\n","word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n","V = len(word_to_idx)\n","T = 13\n","\n","model = CaptioningRNN(word_to_idx,\n","          input_dim=D,\n","          wordvec_dim=W,\n","          hidden_dim=H,\n","          cell_type='rnn',\n","          dtype=np.float64)\n","\n","# Set all model parameters to fixed values\n","for k, v in model.params.items():\n","    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n","\n","features = np.linspace(-1.5, 0.3, num=(N * D)).reshape(N, D)\n","captions = (np.arange(N * T) % V).reshape(N, T)\n","\n","loss, grads = model.loss(features, captions)\n","expected_loss = 9.83235591003\n","\n","print('loss: ', loss)\n","print('expected loss: ', expected_loss)\n","print('difference: ', abs(loss - expected_loss))"]},{"cell_type":"markdown","metadata":{"id":"R-kH_pJlY13Q"},"source":["Запустите следующую ячейку, чтобы выполнить проверку числового градиента в классе `CaptioningRNN`; вы должны увидеть ошибки порядка `e-6` или меньше."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVXYFipGY13Q"},"outputs":[],"source":["np.random.seed(231)\n","\n","batch_size = 2\n","timesteps = 3\n","input_dim = 4\n","wordvec_dim = 5\n","hidden_dim = 6\n","word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n","vocab_size = len(word_to_idx)\n","\n","captions = np.random.randint(vocab_size, size=(batch_size, timesteps))\n","features = np.random.randn(batch_size, input_dim)\n","\n","model = CaptioningRNN(word_to_idx,\n","          input_dim=input_dim,\n","          wordvec_dim=wordvec_dim,\n","          hidden_dim=hidden_dim,\n","          cell_type='rnn',\n","          dtype=np.float64,\n","        )\n","\n","loss, grads = model.loss(features, captions)\n","\n","for param_name in sorted(grads):\n","    f = lambda _: model.loss(features, captions)[0]\n","    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n","    e = rel_error(param_grad_num, grads[param_name])\n","    print('%s relative error: %e' % (param_name, e))"]},{"cell_type":"markdown","metadata":{"id":"d8jDHDhJY13Q"},"source":["# Overfit small data\n","\n","Подобно классу `Solver`, который мы использовали для обучения моделей классификации изображений в предыдущем задании, в этом задании мы используем класс `CaptioningSolver` для обучения моделей подписывающих изображения. Откройте файл `cs231n/captioning_solver.py` и прочитайте класс `CaptioningSolver`; это должно выглядеть очень знакомо.\n","\n","После того, как вы ознакомились с API, выполните следующее, чтобы убедиться, что ваша модель переобучается на небольшой выборке из 100 обучающих примеров. Вы должны увидеть окончательную потерю менее 0.1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"np3mBsp4Y13R"},"outputs":[],"source":["np.random.seed(231)\n","\n","small_data = load_coco_data(max_train=50)\n","\n","small_rnn_model = CaptioningRNN(\n","          cell_type='rnn',\n","          word_to_idx=data['word_to_idx'],\n","          input_dim=data['train_features'].shape[1],\n","          hidden_dim=512,\n","          wordvec_dim=256,\n","        )\n","\n","small_rnn_solver = CaptioningSolver(small_rnn_model, small_data,\n","           update_rule='adam',\n","           num_epochs=50,\n","           batch_size=25,\n","           optim_config={\n","             'learning_rate': 5e-3,\n","           },\n","           lr_decay=0.95,\n","           verbose=True, print_every=10,\n","         )\n","\n","small_rnn_solver.train()\n","\n","# Plot the training losses\n","plt.plot(small_rnn_solver.loss_history)\n","plt.xlabel('Iteration')\n","plt.ylabel('Loss')\n","plt.title('Training loss history')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"34umaPCbY13R"},"source":["Распечатайте окончательный training loss. Вы должны увидеть окончательную потерю менее 0.1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnn_final_training_loss"},"outputs":[],"source":["print('Final loss: ', small_rnn_solver.loss_history[-1])"]},{"cell_type":"markdown","metadata":{"id":"38rl5n2NY13R"},"source":["# Test-time sampling\n","В отличие от моделей классификации, модели подписей к изображениям ведут себя сильно различно во время обучения и во время тестирования. Во время обучения у нас есть доступ к достоверной подписи, поэтому мы передаем достоверные слова в качестве входных данных для RNN на каждом временном шаге. Во время тестирования мы делаем выборку из распределения по словарю на каждом временном шаге и подаем образец в качестве входных данных для RNN на следующем временном шаге.\n","\n","В файле `cs231n/classifiers/rnn.py` реализуйте метод `sample` для выборки во время тестирования. После этого запустите следующее, чтобы выбрать из вашей переобученной модели как данные обучения (train), так и данные проверки (val). Образцы обучающих данных должны быть очень хорошими; образцы данных проверки, вероятно, не будут иметь смысла."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"0jgm0GroY13S"},"outputs":[],"source":["for split in ['train', 'val']:\n","    minibatch = sample_coco_minibatch(small_data, split=split, batch_size=2)\n","    gt_captions, features, urls = minibatch\n","    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n","\n","    sample_captions = small_rnn_model.sample(features)\n","    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n","\n","    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n","        plt.imshow(image_from_url(url))\n","        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n","        plt.axis('off')\n","        plt.show()"]},{"cell_type":"markdown","metadata":{"tags":["pdf-inline"],"id":"wj8L5dJZY13S"},"source":["# INLINE QUESTION 1\n","\n","В нашей текущей настройке подписывания наша языковая модель RNN создает слово на каждом временном шаге в качестве вывода. Однако альтернативный способ постановки проблемы состоит в том, чтобы научить сеть работать с _characters_ (например, «a», «b» и т. д.), а не со словами, чтобы на каждом временном шаге она получала предыдущий символ в качестве входных данных. и пытается предсказать следующий символ в последовательности. Например, сеть может сгенерировать заголовок, например\n","\n","'A', ' ', 'c', 'a', 't', ' ', 'o', 'n', ' ', 'a', ' ', 'b', 'e', 'd'\n","\n","Можете ли вы описать одно из преимуществ модели подписей к изображениям, в которой используется RNN на уровне символов? Можете ли вы также описать один недостаток? СОВЕТ: есть несколько правильных ответов, но может быть полезно сравнить пространство параметров моделей на уровне слов и на уровне символов.\n","\n","**Your Answer:** \n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}