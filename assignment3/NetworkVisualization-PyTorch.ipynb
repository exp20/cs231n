{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_3mBwJCMoTgD"},"outputs":[],"source":["# this mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = None\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","%cd drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n","!bash get_datasets.sh\n","%cd /content"]},{"cell_type":"markdown","metadata":{"collapsed":true,"tags":["pdf-title"],"id":"wwnlUbdcoTgJ"},"source":["# Network Visualization (PyTorch)\n","\n","В этой записной книжке мы рассмотрим использование *градиентов изображения* для создания новых изображений. \n","\n","При обучении модели мы определяем функцию потерь, которая измеряет наше текущее недовольство эффективностью модели; затем мы используем обратное распространение для вычисления градиента потерь по отношению к параметрам модели и выполняем градиентный спуск по параметрам модели, чтобы минимизировать потери. \n","\n","Здесь мы поступим немного иначе. Мы начнем с модели сверточной нейронной сети, которая была предварительно обучена для выполнения классификации изображений в наборе данных ImageNet. Мы будем использовать эту модель, чтобы определить функцию потерь, которая количественно определяет наше текущее недовольство нашим изображением, а затем использовать обратное распространение для вычисления градиента этих потерь по отношению к пикселям изображения. Затем мы сохраним модель фиксированной и выполним градиентный спуск *на изображении*, чтобы синтезировать новое изображение, которое минимизирует потери. \n","\n","В этой записной книжке мы рассмотрим три метода создания изображений: \n","\n","1. **Saliency Maps**: Карты значимости — это быстрый способ определить, какая часть изображения повлияла на решение о классификации, принятое сетью. \n","2. **Fooling Images**: мы можем исказить входное изображение, чтобы оно выглядело одинаково для людей, но было неправильно классифицировано предварительно обученной сетью. \n","3. **Class Visualization**: мы можем синтезировать изображение, чтобы максимизировать оценку классификации определенного класса; это может дать нам некоторое представление о том, что ищет сеть, когда она классифицирует изображения этого класса. \n","\n","В этом ноутбуке используется **PyTorch**; мы предоставили еще одну записную книжку, в которой исследуются те же концепции в TensorFlow. Вам нужно заполнить только одну из этих двух тетрадей."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["pdf-ignore"],"id":"WHRwc2S3oTgL"},"outputs":[],"source":["import torch\n","import torchvision\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from cs231n.image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n","\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"tags":["pdf-ignore"],"id":"leMhnvM-oTgM"},"source":["### Helper Functions\n","\n","Наша предварительно обученная модель обучалась на предварительно обработанных изображениях путем вычитания среднего значения для каждого цвета и деления на стандартное отклонение для каждого цвета. Мы определяем несколько вспомогательных функций для выполнения и отмены этой предварительной обработки в ```cs23n/net_visualization_pytorch```. Вам не нужно ничего делать здесь."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BE7QejCdoTgM"},"outputs":[],"source":["from cs231n.net_visualization_pytorch import preprocess, deprocess, rescale, blur_image"]},{"cell_type":"markdown","metadata":{"id":"_1gqlGrnoTgN"},"source":["# Pretrained Model\n","\n","Для всех наших экспериментов по созданию изображений мы начнем со сверточной нейронной сети, которая была предварительно обучена для выполнения классификации изображений в ImageNet. Здесь мы можем использовать любую модель, но для целей этого задания мы будем использовать SqueezeNet [1], которая обеспечивает точность, сравнимую с AlexNet, но со значительно уменьшенным количеством параметров и вычислительной сложностью.\n","\n","Использование SqueezeNet вместо AlexNet, VGG или ResNet означает, что мы можем легко проводить все эксперименты по созданию изображений на CPU. \n","\n","[1] Iandola et al, «SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size», arXiv 2016"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHOjGDVaoTgO"},"outputs":[],"source":["# Download and load the pretrained SqueezeNet model.\n","model = torchvision.models.squeezenet1_1(pretrained=True)\n","\n","# We don't want to train the model, so tell PyTorch not to compute gradients\n","# with respect to model parameters.\n","for param in model.parameters():\n","    param.requires_grad = False\n","    \n","# you may see warning regarding initialization deprecated, that's fine, please continue to next steps"]},{"cell_type":"markdown","metadata":{"tags":["pdf-ignore"],"id":"j5YKCJCloTgP"},"source":["## Load some ImageNet images\n","\n","Мы предоставили несколько примеров изображений из проверочного набора набора данных ImageNet ILSVRC 2012 Classification. Чтобы загрузить эти изображения, перейдите в `cs231n/datasets/` и запустите `get_imagenet_val.sh`.\n","\n","Поскольку они взяты из проверочного набора, наша предварительно обученная модель не видела эти изображения во время обучения.\n","\n","Запустите следующую ячейку, чтобы визуализировать некоторые из этих изображений вместе с их метками истинности."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["pdf-ignore"],"id":"DkV2xwj5oTgP"},"outputs":[],"source":["from cs231n.data_utils import load_imagenet_val\n","X, y, class_names = load_imagenet_val(num=5)\n","\n","plt.figure(figsize=(12, 6))\n","for i in range(5):\n","    plt.subplot(1, 5, i + 1)\n","    plt.imshow(X[i])\n","    plt.title(class_names[y[i]])\n","    plt.axis('off')\n","plt.gcf().tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"lF3DyFdjoTgQ"},"source":["# Saliency Maps\n","Используя эту предварительно обученную модель, мы будем вычислять карты значимости классов, как описано в разделе 3.1 книги [2].\n","\n","**Карта значимости** сообщает нам, в какой степени каждый пиксель изображения влияет на классификационную оценку этого изображения. Чтобы вычислить это, мы вычисляем градиент ненормализованной оценки, соответствующей правильному классу (который является скаляром) по отношению к пикселям изображения. Если изображение имеет форму `(3, H, W)`, то этот градиент также будет иметь форму `(3, H, W)`; для каждого пикселя изображения этот градиент сообщает нам величину, на которую изменится оценка классификации, если пиксель изменится на небольшую величину. Чтобы вычислить карту значимости, мы берем абсолютное значение этого градиента, затем берем максимальное значение по 3 входным каналам; Таким образом, окончательная карта значимости имеет форму `(H, W)`, и все записи неотрицательны.\n","\n","[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n","Image Classification Models and Saliency Maps\", ICLR Workshop 2014."]},{"cell_type":"markdown","metadata":{"tags":["pdf-ignore"],"id":"-dLoOnKyoTgQ"},"source":["### Hint: PyTorch `gather` method\n","\n","Напомним, что в задании 1 вам нужно было выбрать по одному элементу из каждой строки матрицы; если `s` представляет собой nampy массив формы `(N, C)`, а `y` представляет собой numpy массив формы `(N,`), содержащий целые числа `0 <= y[i] <C`, то `s [np.arange(N), y]` представляет собой массив numpy формы `(N,)`, который выбирает один элемент из каждого элемента в `s`, используя индексы в `y`.\n","\n","\n","В PyTorch вы можете выполнить ту же операцию, используя метод `gather()`. Если `s` является тензором PyTorch формы `(N, C)`, а `y` является тензором PyTorch формы `(N,)`, содержащим long значения в диапазоне `0 <= y[i] <C`, тогда \n","\n","`s.gather(1, y.view(-1, 1)).squeeze()` \n","\n","будет тензором PyTorch формы `(N,)`, содержащим по одной записи из каждой строки `s`, выбранной в соответствии с индексы в `y`. \n","\n","запустите следующую ячейку, чтобы увидеть пример. \n","\n","Вы также можете прочитать документацию по [методу gather](http://pytorch.org/docs/torch.html#torch.gather) и [методу squeeze](http://pytorch.org/docs/torch. html#torch.squeeze)."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["pdf-ignore"],"id":"6a7--V7eoTgR"},"outputs":[],"source":["# Example of using gather to select one entry from each row in PyTorch\n","def gather_example():\n","    N, C = 4, 5\n","    s = torch.randn(N, C)\n","    y = torch.LongTensor([1, 2, 1, 3])\n","    print(s)\n","    print(y)\n","    print(s.gather(1, y.view(-1, 1)).squeeze())\n","gather_example()"]},{"cell_type":"markdown","metadata":{"id":"jD_JrvntoTgS"},"source":["Реализуйте ```compute_saliency_maps``` функцию внутри ```cs231n/net_visualization_pytorch.py```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnR0g9KioTgS"},"outputs":[],"source":["# Load saliency maps computation function\n","from cs231n.net_visualization_pytorch import compute_saliency_maps"]},{"cell_type":"markdown","metadata":{"id":"trrsdIO-oTgS"},"source":["\n","После того, как вы завершили реализацию выше, запустите следующее, чтобы визуализировать некоторые карты значимости классов на наших примерах изображений из набора проверки ImageNet:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2I9u1-4aoTgS"},"outputs":[],"source":["def show_saliency_maps(X, y):\n","    # Convert X and y from numpy arrays to Torch Tensors\n","    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n","    y_tensor = torch.LongTensor(y)\n","\n","    # Compute saliency maps for images in X\n","    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n","\n","    # Convert the saliency map from Torch Tensor to numpy array and show images\n","    # and saliency maps together.\n","    saliency = saliency.numpy()\n","    N = X.shape[0]\n","    for i in range(N):\n","        plt.subplot(2, N, i + 1)\n","        plt.imshow(X[i])\n","        plt.axis('off')\n","        plt.title(class_names[y[i]])\n","        plt.subplot(2, N, N + i + 1)\n","        plt.imshow(saliency[i], cmap=plt.cm.hot)\n","        plt.axis('off')\n","        plt.gcf().set_size_inches(12, 5)\n","    plt.show()\n","\n","show_saliency_maps(X, y)"]},{"cell_type":"markdown","metadata":{"tags":["pdf-inline"],"id":"9cXyCuGuoTgT"},"source":["# INLINE QUESTION\n","\n","Ваш друг предлагает, чтобы найти изображение, которое максимизирует правильную оценку, мы можем выполнить градиентное восхождение по входному изображению, но вместо градиента мы можем фактически использовать карту значимости на каждом шаге для обновления изображения. Верно ли это утверждение? Почему или почему нет?\n","\n","**Your Answer:** \n","\n"]},{"cell_type":"markdown","metadata":{"id":"dLFEHMtToTgT"},"source":["# Fooling Images\n","\n","Мы также можем использовать градиенты изображения для создания «обманчивых изображений», как обсуждалось в [3]. Имея изображение и целевой класс, мы можем выполнить градиентное **восхождение** по изображению, чтобы максимизировать целевой класс, останавливаясь, когда сеть классифицирует изображение как целевой класс. Реализуйте следующую функцию для создания обманчивых изображений. \n","\n","[3] Сегеди и др., «Интересные свойства нейронных сетей», ICLR 2014. \n","\n","Реализуйте функцию `make_fooling_image` внутри `cs231n/net_visualization_pytorch.py.`"]},{"cell_type":"markdown","metadata":{"tags":["pdf-ignore"],"id":"EpVJw3W2oTgT"},"source":["Запустите следующую ячейку, чтобы создать обманчивое изображение. В идеале вы не должны видеть на первый взгляд существенной разницы между исходным и обманчивым изображениями, и теперь сеть должна сделать неверный прогноз относительно обманчивого изображения. Однако вы должны увидеть небольшой случайный шум, если посмотрите на увеличенную в 10 раз разницу между исходным и ложным изображениями. Не стесняйтесь изменять переменную `idx`, чтобы исследовать другие изображения."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uGru8_bEoTgU"},"outputs":[],"source":["from cs231n.net_visualization_pytorch import make_fooling_image\n","idx = 0\n","target_y = 6\n","\n","X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n","X_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\n","\n","scores = model(X_fooling)\n","assert target_y == scores.data.max(1)[1][0].item(), 'The model is not fooled!'"]},{"cell_type":"markdown","metadata":{"id":"08dtWAzmoTgU"},"source":["После создания обманчивого изображения запустите следующую ячейку, чтобы визуализировать исходное изображение, обманчивое изображение, а также разницу между ними."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1XYaAeDoTgU"},"outputs":[],"source":["X_fooling_np = deprocess(X_fooling.clone())\n","X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n","\n","plt.subplot(1, 4, 1)\n","plt.imshow(X[idx])\n","plt.title(class_names[y[idx]])\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 2)\n","plt.imshow(X_fooling_np)\n","plt.title(class_names[target_y])\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 3)\n","X_pre = preprocess(Image.fromarray(X[idx]))\n","diff = np.asarray(deprocess(X_fooling - X_pre, should_rescale=False))\n","plt.imshow(diff)\n","plt.title('Difference')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 4)\n","diff = np.asarray(deprocess(10 * (X_fooling - X_pre), should_rescale=False))\n","plt.imshow(diff)\n","plt.title('Magnified difference (10x)')\n","plt.axis('off')\n","\n","plt.gcf().set_size_inches(12, 5)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8LXWy9qboTgV"},"source":["# Class visualization\n","Начав с изображения случайного шума и выполнив градиентное восхождение для целевого класса, мы можем сгенерировать изображение, которое сеть распознает как целевой класс. Впервые эта идея была представлена в [2]; [3] расширили эту идею, предложив несколько методов регуляризации, которые могут улучшить качество генерируемого изображения. \n","\n","Конкретно, пусть $I$ будет образом, а $y$ будет целевым классом. Пусть $s_y(I)$ будет оценкой, которую сверточная сеть присваивает изображению $I$ для класса $y$; обратите внимание, что это необработанные ненормализованные оценки, а не вероятности класса. Мы хотим сгенерировать изображение $I^*$, которое наберет высокий балл для класса $y$, решив задачу\n","\n","$$\n","I^* = \\arg\\max_I (s_y(I) - R(I))\n","$$\n","\n","где $R$ — регуляризатор (возможно, неявный) (обратите внимание на знак $R(I)$ в argmax: мы хотим минимизировать этот член регуляризации). Мы можем решить эту проблему оптимизации, используя градиентное восхождение, вычисляя градиенты по отношению к сгенерированному изображению. Мы будем использовать (явную) регуляризацию L2 вида\n","\n","$$\n","R(I) = \\lambda \\|I\\|_2^2\n","$$\n","\n","**и** неявная регуляризация, предложенная в [3], путем периодического размытия сгенерированного изображения. Мы можем решить эту проблему, используя градиентное восхождение на сгенерированном изображении.\n","\n","[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n","Image Classification Models and Saliency Maps\", ICLR Workshop 2014.\n","\n","[3] Yosinski et al, \"Understanding Neural Networks Through Deep Visualization\", ICML 2015 Deep Learning Workshop"]},{"cell_type":"markdown","metadata":{"id":"Nt2eiMcLoTgV"},"source":["\n","В `cs231n/net_visualization_pytorch.py` завершите реализацию `image_visualization_update_step`, используемую в функции `create_class_visualization` ниже. После того, как вы завершили эту реализацию, запустите следующие ячейки, чтобы сгенерировать изображение тарантула:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2-mjzpioTgV"},"outputs":[],"source":["from cs231n.net_visualization_pytorch import class_visualization_update_step, jitter, blur_image\n","def create_class_visualization(target_y, model, dtype, **kwargs):\n","    \"\"\"\n","    Generate an image to maximize the score of target_y under a pretrained model.\n","\n","    Inputs:\n","    - target_y: Integer in the range [0, 1000) giving the index of the class\n","    - model: A pretrained CNN that will be used to generate the image\n","    - dtype: Torch datatype to use for computations\n","\n","    Keyword arguments:\n","    - l2_reg: Strength of L2 regularization on the image\n","    - learning_rate: How big of a step to take\n","    - num_iterations: How many iterations to use\n","    - blur_every: How often to blur the image as an implicit regularizer\n","    - max_jitter: How much to gjitter the image as an implicit regularizer\n","    - show_every: How often to show the intermediate result\n","    \"\"\"\n","    model.type(dtype)\n","    l2_reg = kwargs.pop('l2_reg', 1e-3)\n","    learning_rate = kwargs.pop('learning_rate', 25)\n","    num_iterations = kwargs.pop('num_iterations', 100)\n","    blur_every = kwargs.pop('blur_every', 10)\n","    max_jitter = kwargs.pop('max_jitter', 16)\n","    show_every = kwargs.pop('show_every', 25)\n","\n","    # Randomly initialize the image as a PyTorch Tensor, and make it requires gradient.\n","    img = torch.randn(1, 3, 224, 224).mul_(1.0).type(dtype).requires_grad_()\n","\n","    for t in range(num_iterations):\n","        # Randomly jitter the image a bit; this gives slightly nicer results\n","        ox, oy = random.randint(0, max_jitter), random.randint(0, max_jitter)\n","        img.data.copy_(jitter(img.data, ox, oy))\n","        class_visualization_update_step(img, model, target_y, l2_reg, learning_rate)\n","        # Undo the random jitter\n","        img.data.copy_(jitter(img.data, -ox, -oy))\n","\n","        # As regularizer, clamp and periodically blur the image\n","        for c in range(3):\n","            lo = float(-SQUEEZENET_MEAN[c] / SQUEEZENET_STD[c])\n","            hi = float((1.0 - SQUEEZENET_MEAN[c]) / SQUEEZENET_STD[c])\n","            img.data[:, c].clamp_(min=lo, max=hi)\n","        if t % blur_every == 0:\n","            blur_image(img.data, sigma=0.5)\n","\n","        # Periodically show the image\n","        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n","            plt.imshow(deprocess(img.data.clone().cpu()))\n","            class_name = class_names[target_y]\n","            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n","            plt.gcf().set_size_inches(4, 4)\n","            plt.axis('off')\n","            plt.show()\n","\n","    return deprocess(img.data.cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"yE9ObyIhoTgW"},"outputs":[],"source":["dtype = torch.FloatTensor\n","# dtype = torch.cuda.FloatTensor # Uncomment this to use GPU\n","model.type(dtype)\n","\n","target_y = 76 # Tarantula\n","# target_y = 78 # Tick\n","# target_y = 187 # Yorkshire Terrier\n","# target_y = 683 # Oboe\n","# target_y = 366 # Gorilla\n","# target_y = 604 # Hourglass\n","out = create_class_visualization(target_y, model, dtype)"]},{"cell_type":"markdown","metadata":{"id":"rn1yJzS5oTgW"},"source":["\n","Попробуйте визуализацию класса на других классах! Вы также можете свободно экспериментировать с различными гиперпараметрами, чтобы попытаться улучшить качество сгенерированного изображения, но это не обязательно."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGoR1hMqoTgW"},"outputs":[],"source":["# target_y = 78 # Tick\n","# target_y = 187 # Yorkshire Terrier\n","# target_y = 683 # Oboe\n","# target_y = 366 # Gorilla\n","# target_y = 604 # Hourglass\n","target_y = np.random.randint(1000)\n","print(class_names[target_y])\n","X = create_class_visualization(target_y, model, dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s67ZMGK9oTgW"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}